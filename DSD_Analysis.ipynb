{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import utils.protector as protect\n",
    "from utils.cli_utils import softmax_ent\n",
    "from utils.tent import Tent, configure_model, collect_params\n",
    "from utils.utilities import *\n",
    "from utils.plotting import *\n",
    "from typing import Sequence, Tuple, Dict, Optional\n",
    "import argparse\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "CORRUPTIONS = (\n",
    "    \"shot_noise\",\n",
    "    \"motion_blur\",\n",
    "    \"snow\",\n",
    "    \"pixelate\",\n",
    "    \"gaussian_noise\",\n",
    "    \"defocus_blur\",\n",
    "    \"brightness\",\n",
    "    \"fog\",\n",
    "    \"zoom_blur\",\n",
    "    \"frost\",\n",
    "    \"glass_blur\",\n",
    "    \"impulse_noise\",\n",
    "    \"contrast\",\n",
    "    \"jpeg_compression\",\n",
    "    \"elastic_transform\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ENTER PARAMETERS ##\n",
    "\n",
    "# Manual settings for arguments\n",
    "args = type(\"Args\", (), {})()  # Create a simple namespace object\n",
    "args.device = \"cpu\"  # Change this manually as needed\n",
    "args.method = \"none\"  # Options: 'none' or 'tent'\n",
    "args.corruption = \"gaussian_noise\"  # Choose from CORRUPTIONS\n",
    "args.all_corruptions = False  # Set to True to test all corruptions\n",
    "args.n_examples = 1000\n",
    "args.batch_size = 64\n",
    "\n",
    "# Set torch seed for replicability (don't know if this preserves consistency when using different devices)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic set up where we load clean CIFAR-10 and then test on corrupted version. This is a good reference to get a feel for how everyting works together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dynamically set device to best available option\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "# Define normalization transform using CIFAR-10 mean and std values\n",
    "transform = transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2471, 0.2435, 0.2616))\n",
    "\n",
    "# Load pre-trained model and move to appropriate device\n",
    "print(\"Loading model...\")\n",
    "model = get_model(device)\n",
    "\n",
    "# Load clean CIFAR-10 test data to compute source entropies\n",
    "print(\"Loading clean CIFAR-10 as source entropy\")\n",
    "clean_ds = torchvision.datasets.CIFAR10(\n",
    "    root=\"./data\", train=False, download=True, transform=transforms.Compose([transforms.ToTensor(), transform])\n",
    ")\n",
    "clean_loader = DataLoader(clean_ds, batch_size=args.batch_size, shuffle=False)\n",
    "source_ents, accuracy, logits_list, labels_list = evaluate(model, clean_loader, device)\n",
    "\n",
    "# Initialize protector with source entropies for shift detection\n",
    "protector = protect.get_protector_from_ents(\n",
    "    source_ents, argparse.Namespace(gamma=1 / (8 * np.sqrt(3)), eps_clip=1.8, device=device)\n",
    ")\n",
    "\n",
    "# Test model on corrupted datasets\n",
    "corruptions = CORRUPTIONS if args.all_corruptions else [args.corruption]\n",
    "entropy_streams, accs, logits_list_dict, logits_labels_dict = {}, {}, {}, {}\n",
    "\n",
    "# Iterate through each corruption type and severity level\n",
    "for corruption in corruptions:\n",
    "    for severity in range(1, 6):\n",
    "        print(f\"{corruption} severity {severity}\")\n",
    "        # Load corrupted CIFAR-10 data\n",
    "        x, y = load_cifar10c(args.n_examples, severity, corruption)\n",
    "        dataset = BasicDataset(x, y, transform=transform)\n",
    "        loader = DataLoader(dataset, batch_size=args.batch_size, shuffle=False)\n",
    "\n",
    "        # Evaluate model on corrupted data\n",
    "        ents, acc, logits_list, labels_list = evaluate(model, loader, device)\n",
    "        key = f\"{corruption}_s{severity}\"\n",
    "        entropy_streams[key] = ents\n",
    "        accs[key] = acc\n",
    "\n",
    "        logits_list_dict[key] = logits_list\n",
    "        logits_labels_dict[key] = labels_list\n",
    "\n",
    "# Run martingale-based shift detection on entropy streams\n",
    "results = run_martingale(entropy_streams, protector)\n",
    "\n",
    "# Print accuracy summary for all tested corruptions\n",
    "print(\"\\nAccuracy summary:\")\n",
    "for key in accs:\n",
    "    print(f\"{key}: {accs[key] * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through each corruption type\n",
    "for corruption in corruptions:\n",
    "    # For each severity level (1-5)\n",
    "    for severity in range(1, 6):\n",
    "        # Create a key in format \"corruption_name_sX\" where X is severity level\n",
    "        key = f\"{corruption}_s{severity}\"\n",
    "\n",
    "        # Compute accuracy over time from stored logits and labels for this corruption/severity\n",
    "        # This gives us a sequence of accuracies showing how model performance changes\n",
    "        # as it processes more examples from the corrupted dataset\n",
    "        accs = compute_accuracy_over_time_from_logits(logits_list_dict[key], logits_labels_dict[key])\n",
    "\n",
    "        # Store the accuracy sequence in the results dictionary\n",
    "        # Each key maps to a dictionary containing various metrics including:\n",
    "        # - log_sj: martingale wealth values\n",
    "        # - eps: epsilon values\n",
    "        # - accs: accuracy over time\n",
    "        results[key][\"accs\"] = accs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Experiment 1: Plotting Martingale values , epsilon, model accuracy and entropy to see if shift if values line up together**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accs, entropy_streams, logits_list_dict, logits_labels_dict = {}, {}, {}, {}\n",
    "\n",
    "\n",
    "for corruption in corruptions:\n",
    "    for severity in range(1, 6):\n",
    "        print(f\"ðŸ”Ž {corruption} severity {severity} (clean â†’ corrupt)\")\n",
    "\n",
    "        loader, is_clean, labels = load_clean_then_corrupt_sequence(\n",
    "            corruption=corruption,\n",
    "            severity=severity,\n",
    "            n_examples=4000,\n",
    "            data_dir=\"./data\",\n",
    "            transform=transform,\n",
    "            batch_size=args.batch_size,\n",
    "        )\n",
    "\n",
    "        ents, acc, logits_list, labels_list = evaluate(model, loader, device)\n",
    "\n",
    "        key = f\"{corruption}_s{severity}\"\n",
    "        entropy_streams[key] = ents\n",
    "        accs[key] = acc\n",
    "        logits_list_dict[key] = logits_list\n",
    "        logits_labels_dict[key] = labels_list\n",
    "\n",
    "results = {}\n",
    "results = run_martingale(entropy_streams, protector)\n",
    "\n",
    "for corruption in corruptions:\n",
    "    for severity in range(1, 6):\n",
    "        key = f\"{corruption}_s{severity}\"\n",
    "        accs = compute_accuracy_over_time_from_logits(logits_list_dict[key], logits_labels_dict[key])\n",
    "        results[key][\"accs\"] = accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get max log_sj values for each severity level\n",
    "max_log_sj = {}\n",
    "for severity in results:\n",
    "    max_log_sj[severity] = max(results[severity]['log_sj'])\n",
    "    print(f\"Max log_sj for {severity}: {max_log_sj[severity]}\")\n",
    "\n",
    "# Sort by severity level for cleaner display\n",
    "sorted_max_log_sj = dict(sorted(max_log_sj.items()))\n",
    "\n",
    "# Print results\n",
    "for severity, value in sorted_max_log_sj.items():\n",
    "    print(f\"{severity}: {value:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(results['gaussian_noise_s1']['eps'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionaries to store values for each severity\n",
    "log_sj_dict = {}\n",
    "epsilons_dict = {}\n",
    "accuracies_dict = {}\n",
    "\n",
    "# Extract values for each severity level\n",
    "for severity in range(1, 6):\n",
    "    key = f\"gaussian_noise_s{severity}\"\n",
    "    if key in results:\n",
    "        log_sj_dict[key] = results[key][\"log_sj\"]\n",
    "        epsilons_dict[key] = results[key][\"eps\"]\n",
    "        accuracies_dict[key] = results[key][\"accs\"]\n",
    "\n",
    "# Plot the combined results\n",
    "plot_combined_martingale_accuracy_severity(\n",
    "    log_sj_dict,\n",
    "    epsilons_dict,\n",
    "    accuracies_dict,\n",
    "    entropy_dict=entropy_streams,\n",
    "    batch_size=64,\n",
    "    title=\"Gaussian Noise Comparison Across Severities\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print sizes of each dictionary\n",
    "print(\"Log SJ Dictionary size:\", len(log_sj_dict))\n",
    "print(\"Epsilons Dictionary size:\", len(epsilons_dict))\n",
    "print(\"Accuracies Dictionary size:\", len(accuracies_dict))\n",
    "\n",
    "# For a single key, print the length of arrays\n",
    "key = f\"gaussian_noise_s1\"\n",
    "if key in log_sj_dict:\n",
    "    print(f\"\\nFor {key}:\")\n",
    "    print(f\"log_sj length: {len(log_sj_dict[key])}\")\n",
    "    print(f\"epsilons length: {len(epsilons_dict[key])}\")\n",
    "    print(f\"accuracies length: {len(accuracies_dict[key])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detection_delays = compute_detection_delays_from_threshold(log_sj_dict)\n",
    "\n",
    "# Call the function to plot detection delays\n",
    "plot_detection_delays(detection_delays)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Experiment 2: Fix severity at 5 and change the corruption type to see if shift detector works differently based on corruption type**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.severity = 5  # Fix severity to level 5 for all corruptions\n",
    "\n",
    "accs, entropy_streams, logits_list_dict, logits_labels_dict = {}, {}, {}, {}\n",
    "\n",
    "for corruption in CORRUPTIONS:\n",
    "    print(f\"ðŸ”Ž {corruption} severity {args.severity} (clean â†’ corrupt)\")\n",
    "\n",
    "    loader, is_clean, labels = load_clean_then_corrupt_sequence(\n",
    "        corruption=corruption,\n",
    "        severity=args.severity,\n",
    "        n_examples=4000,\n",
    "        data_dir=\"./data\",\n",
    "        transform=transform,\n",
    "        batch_size=args.batch_size,\n",
    "    )\n",
    "\n",
    "    ents, acc, logits_list, labels_list = evaluate(model, loader, device)\n",
    "\n",
    "    key = f\"{corruption}_s{args.severity}\"\n",
    "    entropy_streams[key] = ents\n",
    "    accs[key] = acc\n",
    "    logits_list_dict[key] = logits_list\n",
    "    logits_labels_dict[key] = labels_list\n",
    "\n",
    "results = run_martingale(entropy_streams, protector)\n",
    "\n",
    "for corruption in CORRUPTIONS:\n",
    "    key = f\"{corruption}_s{args.severity}\"\n",
    "    accs = compute_accuracy_over_time_from_logits(logits_list_dict[key], logits_labels_dict[key])\n",
    "    results[key][\"accs\"] = accs\n",
    "\n",
    "# Create dictionaries to store values for each corruption\n",
    "log_sj_dict = {}\n",
    "epsilons_dict = {}\n",
    "accuracies_dict = {}\n",
    "entropy_dict = {}\n",
    "\n",
    "# Extract values for each corruption at severity level 5\n",
    "for corruption in CORRUPTIONS:\n",
    "    key = f\"{corruption}_s{args.severity}\"\n",
    "    if key in results:\n",
    "        log_sj_dict[key] = results[key][\"log_sj\"]\n",
    "        epsilons_dict[key] = results[key][\"eps\"]\n",
    "        accuracies_dict[key] = results[key][\"accs\"]\n",
    "        entropy_dict[key] = entropy_streams[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the combined results\n",
    "plot_combined_martingale_accuracy_corruption(\n",
    "    log_sj_dict,\n",
    "    epsilons_dict,\n",
    "    accuracies_dict,\n",
    "    entropy_dict=entropy_dict,\n",
    "    batch_size=64,\n",
    "    title=\"Comparison Across Corruption Types at Severity 5\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Experiment 3: Delay analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detection_delays = compute_detection_delays_from_threshold(log_sj_dict)\n",
    "\n",
    "# Plot detection delays\n",
    "plot_detection_delays(detection_delays)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute accuracy drops\n",
    "accuracy_drops = compute_accuracy_drops(accuracies_dict)\n",
    "\n",
    "# Print results to verify\n",
    "for corruption, drop in accuracy_drops.items():\n",
    "    print(f\"{corruption}: {drop:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Experiment 4: Correlation Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Align data\n",
    "corruptions = list(detection_delays.keys())\n",
    "delays = [detection_delays[c] for c in corruptions]\n",
    "drops = [accuracy_drops[c] for c in corruptions]\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(delays, drops, color=\"crimson\")\n",
    "for i, name in enumerate(corruptions):\n",
    "    plt.annotate(name, (delays[i], drops[i]), textcoords=\"offset points\", xytext=(5, 5), ha=\"left\", fontsize=9)\n",
    "\n",
    "plt.title(\"Detection Delay vs Accuracy Drop\")\n",
    "plt.xlabel(\"Detection Delay (samples after corruption)\")\n",
    "plt.ylabel(\"Accuracy Drop (clean - corrupt)\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Negative correlation between detection delay and accuracy drop implies the detector prioritizes responding to impactful shifts, rather than reacting to every minor change. Thatâ€™s exactly what you want in a reliable shift detector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Experiment 5: Clustering Corruptions by Detection Behavior**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reuse data from previous experiment (some of this is redundant and just here for readability)\n",
    "# Could maybe add more features if necessary\n",
    "detection_delays = compute_detection_delays_from_threshold(log_sj_dict)\n",
    "accuracy_drops = compute_accuracy_drops(accuracies_dict)\n",
    "entropy_spikes = compute_entropy_spikes(entropy_streams)\n",
    "confidence_slopes = compute_detection_confidence_slope(log_sj_dict, epsilons_dict)\n",
    "\n",
    "results = list()\n",
    "\n",
    "for corruption in entropy_spikes.keys():\n",
    "    results.append(\n",
    "        {\n",
    "            \"corruption\": corruption,\n",
    "            \"detection_delay\": detection_delays[corruption],\n",
    "            \"accuracy_drop\": accuracy_drops[corruption],\n",
    "            \"entropy_spike\": entropy_spikes[corruption],\n",
    "            \"confidence_slope\": confidence_slopes[corruption],\n",
    "        }\n",
    "    )\n",
    "\n",
    "clustering_df = pd.DataFrame(results)\n",
    "# print(clustering_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features for clustering\n",
    "feature_cols = [col for col in clustering_df.columns if col not in [\"corruption\", \"tsne_1\", \"tsne_2\"]]\n",
    "X = clustering_df[feature_cols].values\n",
    "\n",
    "\n",
    "# K-means setup - 4 clusters seems considering the result\n",
    "n_clusters = 4\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "clustering_df[\"cluster\"] = kmeans.fit_predict(X)\n",
    "\n",
    "# TSNE for 2D visualization\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=5)\n",
    "X_tsne = tsne.fit_transform(X)\n",
    "clustering_df[\"tsne_1\"] = X_tsne[:, 0]\n",
    "clustering_df[\"tsne_2\"] = X_tsne[:, 1]\n",
    "\n",
    "\n",
    "# Plot things\n",
    "plt.figure(figsize=(10, 7))\n",
    "colors = plt.cm.rainbow(np.linspace(0, 1, n_clusters))\n",
    "\n",
    "for i in range(n_clusters):\n",
    "    subset = clustering_df[clustering_df[\"cluster\"] == i]\n",
    "    plt.scatter(subset[\"tsne_1\"], subset[\"tsne_2\"], color=colors[i], label=f\"Cluster {i}\", s=100)\n",
    "\n",
    "for _, row in clustering_df.iterrows():\n",
    "    plt.text(row[\"tsne_1\"] + 0.5, row[\"tsne_2\"] + 0.5, row[\"corruption\"], fontsize=9)\n",
    "\n",
    "plt.title(\"t-SNE Projection Colored by k-means Cluster\")\n",
    "plt.xlabel(\"t-SNE Dim 1\")\n",
    "plt.ylabel(\"t-SNE Dim 2\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Experiment 6: Label Noise and Concept Shift**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shift experiment setup:\n",
    "# Fix the new priors for replicability\n",
    "new_priors = {\n",
    "    0: 0.3,\n",
    "    1: 0.15,\n",
    "    2: 0.1,\n",
    "    3: 0.1,\n",
    "    4: 0.11,\n",
    "    5: 0.07,\n",
    "    6: 0.05,\n",
    "    7: 0.05,\n",
    "    8: 0.04,\n",
    "    9: 0.03,\n",
    "}\n",
    "\n",
    "# Label Noise: input-label mapping changes: \"dog\" labeled as \"cat\"\n",
    "label_noise_data = apply_label_noise_to_dataset(clean_ds, noise_rate=0.3)\n",
    "# Class Prior Shift: Class proportions changes, no longer even split between classes\n",
    "prior_shift_data = simulate_prior_shift(clean_ds, class_priors=new_priors)\n",
    "# Combine Prior shift and Label Noise\n",
    "combined_shift_data = simulate_prior_shift_with_label_noise(clean_ds, class_priors=new_priors)\n",
    "\n",
    "shift_experiments = {\n",
    "    \"label noise\": label_noise_data,\n",
    "    \"prior shift\": prior_shift_data,\n",
    "    \"combined\": combined_shift_data,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can reuse components of experiment 2 here\n",
    "accs, entropy_streams, logits_list_dict, logits_labels_dict = {}, {}, {}, {}\n",
    "\n",
    "for shift, data in shift_experiments.items():\n",
    "    print(f\" Running shift scenario: {shift}\")\n",
    "\n",
    "    # 1. Generate corrupted dataset\n",
    "    corrupted_dataset = data\n",
    "\n",
    "    # 2. Create DataLoader\n",
    "    corrupted_loader = DataLoader(corrupted_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "    # 3. Evaluate model on corrupted data\n",
    "    ents, acc, logits_list, labels_list = evaluate(model, corrupted_loader, device)\n",
    "\n",
    "    key = shift\n",
    "    entropy_streams[shift] = ents\n",
    "    accs[shift] = acc\n",
    "    logits_list_dict[key] = logits_list\n",
    "    logits_labels_dict[key] = labels_list\n",
    "\n",
    "    results = run_martingale(entropy_streams, protector)\n",
    "\n",
    "for shift in shift_experiments.keys():\n",
    "    key = shift\n",
    "    accs = compute_accuracy_over_time_from_logits(logits_list_dict[key], logits_labels_dict[key])\n",
    "    results[key][\"accs\"] = accs\n",
    "\n",
    "# Create dictionaries to store values for each shift\n",
    "log_sj_dict = {}\n",
    "epsilons_dict = {}\n",
    "accuracies_dict = {}\n",
    "entropy_dict = {}\n",
    "\n",
    "# Extract values for each shift\n",
    "for shift in shift_experiments.keys():\n",
    "    if shift in results:\n",
    "        log_sj_dict[shift] = results[shift][\"log_sj\"]\n",
    "        epsilons_dict[shift] = results[shift][\"eps\"]\n",
    "        accuracies_dict[shift] = results[shift][\"accs\"]\n",
    "        entropy_dict[shift] = entropy_streams[shift]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results for shift types\n",
    "plot_combined_noise_shift(\n",
    "    log_sj_dict,\n",
    "    epsilons_dict,\n",
    "    accuracies_dict,\n",
    "    entropy_dict=entropy_dict,\n",
    "    batch_size=64,\n",
    "    title=\"Comparison for Label Noise, Prior Shift, and Combined Shift\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Experiment 7: Continual and Recurring Shifts**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example configuration\n",
    "corruption = \"gaussian_noise\"\n",
    "segment_schedule = [\"clean\", \"s2\", \"s5\", \"clean\", \"s4\"]\n",
    "segment_size = 1500\n",
    "\n",
    "# Load dynamic stream\n",
    "loader, is_clean, labels, seg_labels = load_dynamic_sequence(\n",
    "    segments=segment_schedule,\n",
    "    segment_size=segment_size,\n",
    "    corruption_name=corruption,\n",
    "    data_dir=\"./data\",\n",
    "    transform=transform,\n",
    "    batch_size=args.batch_size,\n",
    ")\n",
    "\n",
    "# Evaluate model over full stream\n",
    "ents, acc, logits_list, labels_list = evaluate(model, loader, device)\n",
    "\n",
    "# Run martingale detection\n",
    "key = f\"{corruption}_dynamic\"\n",
    "entropy_streams = {key: ents}\n",
    "results = run_martingale(entropy_streams, protector)\n",
    "\n",
    "# Compute accuracy over time\n",
    "acc_stream = compute_accuracy_over_time_from_logits(logits_list, labels_list)\n",
    "results[key][\"accs\"] = acc_stream\n",
    "\n",
    "# Collect results for plotting/comparison\n",
    "log_sj_dict = {key: results[key][\"log_sj\"]}\n",
    "epsilons_dict = {key: results[key][\"eps\"]}\n",
    "accuracies_dict = {key: acc_stream}\n",
    "entropy_dict = {key: ents}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dynamic_stream_analysis(\n",
    "    log_sj_dict,\n",
    "    epsilons_dict,\n",
    "    accuracies_dict,\n",
    "    entropy_dict,\n",
    "    segment_schedule=segment_schedule,\n",
    "    segment_size=segment_size,\n",
    "    batch_size=args.batch_size,\n",
    "    title=f\"Dynamic Stream Analysis: {corruption} with Varying Severity\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "poem",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
